<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  
    <meta name="description" content="Heading to entrepreneur." />
  
  
  
    <meta name="keywords" content="technical, entrepreneur, consulting, blog, personal, life, work, freelancer" />
  
  
  
  
  <meta name="robots" content="INDEX,FOLLOW" />

  <title>
    Joshua Chi's Blog &middot; Free To Feel
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/webicon.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <header class="masthead">
      <div class="masthead-inner">
        <h1><a href="http://freetofeel.com">Free To Feel</a></h1>
        <p class="lead">Heading to entrepreneur.</p>
        <br>
        <img src='http://www.gravatar.com/avatar/81759b36bd906bbf803ce4a5ce84e643.png' />Joshua Chi
        <div>
        <a class="webicon twitter small" href="http://twitter.com/Joshua_C/">Twitter</a>
        <a class="webicon github small" href="https://github.com/joshuachi">Github</a>
        </div>
        <div class="colophon">
          <ul class="colophon-links">
            <li><a href="http://www.freetofeel.com/zh/">中文版本</a></li>
            <li><a href="http://www.freetofeel.com/google-calendar-hours/">Google calendar hours calculator</a></li>
            <li><a href='http://www.freetofeel.com/archives.html'> Archived posts</a>
            </li>
            <li><a href="http://freetofeel.com/atom.xml">Subscribe(xml)</a>
            </li>
          </ul>
          <p>Copyright &copy; 2017 Joshua Chi - Powered by <a href="http://github.com/mojombo/jekyll">Jekyll</a> and <a href="http://github.com/">Github</a>
</p>
        </div>                
      </div>
    </header>

    <div class="content container">
      <div id="home">

<div class="post">
  <ul class="content">
		
			<div class="post" id="postid-/en/2016/09/16/PROXYPASS-EVIL">
				<a href="/en/2016/09/16/PROXYPASS-EVIL.html"><h1 class="title">PROXYPASS Evil</h1></a>
				
				<p>You are forced to invent new solution for your business, otherwise you are going to kick your own ass. :-)</p>

<p>Although the post is targed with name "proxypass evil", it is not true.
I like <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html">proxypass</a>, it hides(protects) backend services from outside, especially when we are practicing <a href="http://martinfowler.com/articles/microservices.html">Microservices</a> nowdays.</p>

<p>The post here is explainning a new solution to implement your heavy user experience website, with light backend service and Aliyun services.</p>

<h2>Background</h2>

<p><a href="http://www.aliyun.com">Aliyun(China)</a> service becomes more and more popular since more and more none-technical company transfer their website from their own hosting to Aliyun, the same as Amazon's.</p>

<p><a href="https://cn.aliyun.com/product/oss">OSS</a> could be used to store static resource.</p>

<p><a href="https://cn.aliyun.com/product/cdn">CDN</a> could be used to speed up the resoure loading, by binding with your domain <a href="https://en.wikipedia.org/wiki/CNAME_record">CNAME</a> and uploading website file object to OSS.</p>

<h2>Context</h2>

<ol>
<li>You are giving a Amazon EC2 T2 medium node. Website is full of images and videos for around 1GB.</li>
<li>You are told to make the site loading speed with a duration which human could accept, let's say 30s[1], with an expect a high traffic coming in following days.</li>
<li>There are some small backend services, which frontend needs to talk with.</li>
</ol>


<h3>Let's compare two solutions:</h3>

<p><strong>A</strong>. EC2 node to host site and all static resource requests are proxypassed to Aliyun OSS CDN;</p>

<p><strong>B</strong>. Aliyun OSS CDN to host site and it will talk to backend service hosted at EC2 node;</p>

<p>If you choose <strong>A</strong>, you have to prepare a powerful EC2 node, no matter of bandwidth or CPU, considering your node need to accept all traffic and pass back and forth from OSS/CDN.</p>

<p>If you find solution <strong>B</strong>, you are binding your website with Aliyun service. Binding, I mean, actually your website experience is provided by Aliyun. Only if Aliyun dies, your website is going to die. The only tricky part is if you have backend services need to be integrated, you have to provide CORS in your backend services, considering www.example.com has a CNAME record bound with CDN already.</p>

<p>Is this a cool idea? For me, yes! I can have a good sleep without worrying about our marketing team traffic plan.</p>

<h3>Pains from EC2 or your own hosting.</h3>

<p>In my first day implementing solution <strong>A</strong> in a EC2 node(T2 medium), the site is really fast, let's say 10s. After one day, nothing changes, when I load speed test the website again, it loads for around 1m. Unbelievable that it is "NOT STABLE" for Aliyun CDN. After comparing with solution <strong>B</strong>, it is not the CDN fault.</p>

<p>Amazon EC2 T2 instances has a feature named "<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/t2-instances.html">CPU Credits</a>"</p>

<blockquote><p>When a T2 instance uses fewer CPU resources than its base performance level allows (such as when it is idle), the unused CPU credits (or the difference between what was earned and what was spent) are stored in the credit balance for up to 24 hours, building CPU credits for bursting. When your T2 instance requires more CPU resources than its base performance level allows, it uses credits from the CPU credit balance to burst up to 100% utilization. The more credits your T2 instance has for CPU resources, the more time it can burst beyond its base performance level when more performance is needed.</p></blockquote>

<p>Something to prove it.
<img src="http://freetofeel.com/images/left-cpu-credits.jpg" alt="EC2 T2 Left CPU Credits" /></p>

<p><img src="http://freetofeel.com/images/cpu-credits.jpg" alt="EC2 T2 CPU Credits" /></p>

<p><code>2016.09.12 3:00 UTC</code> - the time the website is really fast, comparing with following days(e.g. 09.13, 09.14), the CPU credits usage never exceeds 0.2.</p>

<p>It is the same reason if you have a poor server hosting by yourself, unless you could prepare a powerful CPU and memory.</p>

<h2>REFs</h2>

<ol>
<li>30s - yes, since I joined this new company, this number is exciting all the creative guys. Sadly to say that we are using more and more cool technologies[e.g. <a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a>] nowdays, but with slower and slower website load speed.</li>
</ol>



				<div class="post-metadata">
					September 16, 2016 | 
					<a href='/en/2016/09/16/PROXYPASS-EVIL.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#nginx'>nginx</a>
						, 
					
						
						<a href='/tags.html#proxypass'>proxypass</a>
						, 
					
						
						<a href='/tags.html#amazon'>amazon</a>
						, 
					
						
						<a href='/tags.html#ec2'>ec2</a>
						, 
					
						
						<a href='/tags.html#cpu-credit'>cpu-credit</a>
						, 
					
						
						<a href='/tags.html#aliyun'>aliyun</a>
						, 
					
						
						<a href='/tags.html#oss'>oss</a>
						, 
					
						
						<a href='/tags.html#cdn'>cdn</a>
						, 
					
						
						<a href='/tags.html#budget'>budget</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2015/12/20/npm-nodejs-best-practice-with-macos">
				<a href="/en/2015/12/20/npm-nodejs-best-practice-with-macos.html"><h1 class="title">NPM NodeJS Environment Setup Best Practice with MACOS</h1></a>
				
				<h3>NPM Installation</h3>

<h4>Do not install NPM with homebrew</h4>

<ul>
<li><a href="http://blog.npmjs.org/post/85484771375/how-to-install-npm">There is an extremely high correlation between "installed npm with Homebrew", and "npm is irreparably broken"</a></li>
<li><a href="https://gist.github.com/DanHerbert/9520689">Explanation of the issue</a></li>
</ul>


<h4>Install it from official site</h4>

<ul>
<li>List all node modules installed globally:</li>
</ul>


<pre>
    npm ls -g --depth=0
</pre>


<ul>
<li>Delete global node_modules folder:</li>
</ul>


<pre>
    sudo rm -rf /usr/local/lib/node_modules
</pre>


<ul>
<li>Unintall Node</li>
</ul>


<pre>
brew uninstall node
or
sudo rm /usr/local/bin/node
</pre>


<ul>
<li>Clean symbol link to node modules globally.</li>
</ul>


<pre>
    cd  /usr/local/bin && ls -l | grep "../lib/node_modules/" | awk '{print $9}'| xargs rm
</pre>


<ul>
<li>Install NPM</li>
</ul>


<pre>
curl -L https://www.npmjs.com/install.sh | sh
</pre>


<h3>Node Installation</h3>

<h4>Install nvm</h4>

<pre>
curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.29.0/install.sh | bash
</pre>


<h4>Switch version</h4>

<pre>
nvm install stable #Install latest stable node
nvm install 4.2.2 #Install 4.2.2 version
nvm install 0.12.7 #Install 0.12.7 version
nvm use 0 #Will switch to use v0.12.7
</pre>


<h4>Use a <a href="https://blog.heroku.com/archives/2015/11/10/node-habits-2016#2-use-a-smart-npmrc">smart .npmrc</a></h4>

<p>By default, npm doesn't save installed dependencies to package.json (and you should always track your dependencies!).</p>

<p>If you use the --save flag to auto-update package.json, npm installs the packages with a leading carat (^), putting your modules at risk of drifting to different versions.</p>

<p>One solution is installing packages like this:</p>

<pre>
$ npm install foobar --save --save-exact
</pre>


<p>Even better, you can set these options in ~/.npmrc to update your defaults:</p>

<pre>
$ npm config set save=true
$ npm config set save-exact=true
$ cat ~/.npmrc
</pre>


<p>Now, npm install foobar will automatically add foobar to package.json and your dependencies won't drift between installs!</p>

<p>If you prefer to keep flexible dependencies in package.json, but still need to lock down dependencies for production, you can alternatively build npm's <a href="https://docs.npmjs.com/cli/shrinkwrap">shrinkwrap</a> into your workflow.
This takes a little more effort, but has the added benefit of preserving exact versions of nested dependencies.</p>

<h4>Dependencies version management</h4>

<pre>
*: Match any verion
1.1.0: Exactly match the version
~1.1.0: >=1.1.0 && < 1.2.0
^1.1.0: >=1.1.0 && < 2.0.0
</pre>


<h4>nrm switch NPM soruce</h4>

<p>http://www.tuicool.com/articles/nYjqeu</p>

<h5>Install</h5>

<pre><code>$ npm install -g nrm
</code></pre>

<h5>Usage</h5>

<p>List all available sources</p>

<pre><code>nrm ls                                                                                                                                    

* npm ---- https://registry.npmjs.org/
cnpm --- http://r.cnpmjs.org/
taobao - http://registry.npm.taobao.org/
eu ----- http://registry.npmjs.eu/
au ----- http://registry.npmjs.org.au/
sl ----- http://npm.strongloop.com/
nj ----- https://registry.nodejitsu.com/
</code></pre>

<p><code>*</code> means source you are currently using</p>

<h5>Switch</h5>

<p>Switch to use the one from taobao</p>

<pre>
nrm use taobao                                                                                                 
</pre>


<p>Registry has been set to: http://registry.npm.taobao.org/</p>

<h4>References</h4>

<ul>
<li><a href="https://blog.heroku.com/archives/2015/11/10/node-habits-2016#2-use-a-smart-npmrc">10 Habits of a Happy Node Hacker (2016)</a></li>
<li><a href="https://github.com/streakq/js-tools-best-practice/blob/master/doc/node.md#nrm-%E5%BF%AB%E9%80%9F%E5%88%87%E6%8D%A2-npm-%E6%BA%90">NRM switch NPM sources</a></li>
</ul>



				<div class="post-metadata">
					December 20, 2015 | 
					<a href='/en/2015/12/20/npm-nodejs-best-practice-with-macos.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#npm'>npm</a>
						, 
					
						
						<a href='/tags.html#nodejs'>nodejs</a>
						, 
					
						
						<a href='/tags.html#nvm'>nvm</a>
						, 
					
						
						<a href='/tags.html#macos'>macos</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2014/12/17/mongodb-readonly-performance-production-tuning">
				<a href="/en/2014/12/17/mongodb-readonly-performance-production-tuning.html"><h1 class="title">Mongodb Read-only Performance Production Tuning</h1></a>
				
				<h3>MongoDB VS Tokumx</h3>

<p>Check this post <a href="http://stackoverflow.com/questions/27359394/tokumx-vs-mongodb-read-performance">Tokumx VS mongodb read-only performance</a> @stackoverflow firstly, which I have already described the context when comparing MongoDB and Tokumx read-only performance.</p>

<p><a href="http://www.tokutek.com/tokumx-for-mongodb/download-community/">TokuMX 2.0.0 Community Edition for MongoDB</a> is still built on MongoDB 2.4 which doesn't have GEO <a href="http://docs.mongodb.org/manual/core/2dsphere/">2dsphere</a> index yet. So the post@stackoverflow might not be fair for <a href="http://www.tokutek.com/tokumx-for-mongodb/">Tokumx</a>. Personally, I like tokumx.</p>

<ul>
<li>storage compress</li>
<li>document level write lock</li>
<li>concurrency write performance</li>
<li>do not have to worry about emebeded document size change</li>
<li>...</li>
</ul>


<p>A lot of those cool features you can find in <a href="http://www.tokutek.com/tokumx-for-mongodb/hdd-benchmarks/">TOKUMX™ BENCHMARK VS. MONGODB – HDD</a> and <a href="http://docs.tokutek.com/tokumx/">Tokumx Documentation</a></p>

<h3>2d index VS 2dsphere index</h3>

<blockquote><p>2d indexes support:</p>

<ul>
<li>Calculations using flat geometry</li>
<li>Legacy coordinate pairs (i.e., geospatial points on a flat coordinate system)</li>
<li>Compound indexes with only one additional field, as a suffix of the 2d index field</li>
</ul>


<p>2dsphere indexes support:</p>

<ul>
<li>Calculations on a sphere</li>
<li>GeoJSON objects and include backwards compatibility for legacy coordinate pairs</li>
<li>Compound indexes with scalar index fields (i.e. ascending or descending) as a prefix or suffix of the 2dsphere index field</li>
</ul>
</blockquote>

<p>So basically if you have multiple fields need to be compound together, you need use 2dshpere index.</p>

<p>Nothing is perfect, if you are doing sorting some fields, like <code>id desc</code>, you will have the [geo query with sort performance] issue(http://stackoverflow.com/questions/12908871/mongodb-geospatial-query-with-sort-performance-issues).</p>

<h3>geoWithin VS near</h3>

<p>So we choose 2dsphere index for our case. But performance matters if you are using <a href="http://docs.mongodb.org/manual/reference/operator/query/geoWithin/#op._S_geoWithin">geoWithin</a> and <a href="http://docs.mongodb.org/manual/reference/operator/query/near/#op._S_near">near</a>.</p>

<pre><code>db.collection.find(
   {
   $query: {
     geo:
     {
       $near :
        {
          $geometry: img.geo,
          $maxDistance: 100000
        }
     },
     gender: 3
   },
   $orderby: { '_pid' : -1 },
   $limit: 3,
   $skip: 1
  }
);

 "cursor" : "S2NearCursor",
 "isMultiKey" : false,
 "n" : 54,
 "nscannedObjects" : 502,
 "nscanned" : 502,
 "nscannedObjectsAllPlans" : 502,
 "nscannedAllPlans" : 502,
 "scanAndOrder" : true,
 "indexOnly" : false,
 "nYields" : 4,
 "nChunkSkips" : 0,
 "millis" : 3,
 "indexBounds" : {
 },


db.collection.find(
  {
    $query: {
      geo : {
        $geoWithin : {
          $centerSphere : [ img.geo.coordinates , 100/3959 ]
        }
      },
      gender:3
    },
    $orderby: { '_pid' : -1 },
    $limit: 3,
    $skip: 1
  }
);

 "cursor" : "BtreeCursor geo_2dsphere_gender_1",
 "isMultiKey" : false,
 "n" : 159,
 "nscannedObjects" : 249,
 "nscanned" : 337,
 "nscannedObjectsAllPlans" : 249,
 "nscannedAllPlans" : 337,
 "scanAndOrder" : true,
 "indexOnly" : false,
 "nYields" : 3,
 "nChunkSkips" : 0,
 "millis" : 3,
 "indexBounds" : {
      "geo" : [],
      "gender" : [
           [
                3,
                3
           ]
      ]
</code></pre>

<p>You can see they are using two different cursor <code>S2NearCursor</code> and <code>BtreeCursor</code>. In our case <code>S2NearCursor</code> works better than <code>BtreeCursor</code>.</p>

<h3>MongoDB and NUMA Hardware</h3>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Non-uniform_memory_access">What's Non-uniform memory access (NUMA)</a>?</li>
<li><a href="http://en.wikipedia.org/wiki/Interleaved_memory">What's Interleaved memory</a></li>
</ul>


<p>NUMA and Interleaved Memory affect MongoDB perofmrnace, you can find below in <a href="http://docs.mongodb.org/manual/administration/production-notes/">Production notes</a>.</p>

<blockquote><ul>
<li><p>Running MongoDB on a system with Non-Uniform Access Memory (NUMA) can cause a number of operational problems, including slow performance for periods of time and high system process usage.</p></li>
<li><p>When running MongoDB servers and clients on NUMA hardware, you should configure a memory interleave policy so that the host behaves in a non-NUMA fashion. MongoDB checks NUMA settings on start up when deployed on Linux (since version 2.0) and Windows (since version 2.6) machines, and prints a warning if the NUMA configuration may degrade performance.</p></li>
<li><p>See The <a href="http://jcole.us/blog/archives/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/">MySQL “swap insanity” problem and the effects of NUMA</a> post, which describes the effects of NUMA on databases. This blog post addresses the impact of NUMA for MySQL, but the issues for MongoDB are similar. The post introduces NUMA and its goals, and illustrates how these goals are not compatible with production databases.</p></li>
</ul>
</blockquote>

<h3>Tunning client performance</h3>

<p>In the /etc/sysctl.conf file:</p>

<blockquote><ul>
<li>net.ipv4.tcp_tw_recycle = 1</li>
<li>net.ipv4.tcp_tw_reuse = 1</li>
</ul>
</blockquote>

<ul>
<li><p>TCP_TW_RECYCLE Description: Enables fast recycling of TIME_WAIT sockets. Use with caution and ONLY in internal network where network connectivity speeds are “faster”.</p></li>
<li><p>TCP_TW_REUSE Description: Allows for reuse of sockets in TIME_WAIT state for new connections, only when it is safe from the network stack’s perspective.</p></li>
</ul>



				<div class="post-metadata">
					December 17, 2014 | 
					<a href='/en/2014/12/17/mongodb-readonly-performance-production-tuning.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#mongodb'>mongodb</a>
						, 
					
						
						<a href='/tags.html#tokumx'>tokumx</a>
						, 
					
						
						<a href='/tags.html#cowboy'>cowboy</a>
						, 
					
						
						<a href='/tags.html#erlang'>erlang</a>
						, 
					
						
						<a href='/tags.html#bson'>bson</a>
						, 
					
						
						<a href='/tags.html#tsung'>tsung</a>
						, 
					
						
						<a href='/tags.html#performance'>performance</a>
						, 
					
						
						<a href='/tags.html#concurrency'>concurrency</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2014/04/20/solr-performance-issue-solving-process">
				<a href="/en/2014/04/20/solr-performance-issue-solving-process.html"><h1 class="title">Some suggestions when you work with solr</h1></a>
				
				<h2>Background</h2>

<p>Since we deploy solr to production, it was running fine for first few days, and then some day it will be slow to response. It happened at least several times like this. We can start this by analyzing from this graphite screenshot.</p>

<p><a href="http://freetofeel.com/images/solr-prod-slow.png"><img src="http://freetofeel.com/images/solr-prod-slow_s.png" alt="Solr Graphite screenshot" /></a></p>

<h3>The whole server strucuture:</h3>

<ul>
<li>We have three solr instances: one master and two slave, which managed by zookeeper</li>
<li>All client requests will be queued into <code>queue.size.solr</code> firstly</li>
</ul>


<p><code>Notice: we will not discuss the strucuture was designed correctly or not in this post. This blog will just focus on how to use solr itself.</code></p>

<h3>More info about this graphite</h3>

<ul>
<li><code>queue.size.solr</code> was increased since <code>solr.request.time.avg</code> increased, which is obvious;</li>
<li>The increasing of <code>solr.request.time.avg</code> was not caused by <code>solr.requests</code> which is quite stable during <code>Fri 12PM</code> to <code>Fri 8PM</code>;</li>
<li>Please ignore the drop of <code>system.memory.Memfree</code> which is caused by a restart of solr instance on master(BTW, we stored index in RAM);</li>
</ul>


<h3>A warning from production solr log</h3>

<blockquote><p>PERFORMANCE WARNING: Overlapping onDeckSearchers=X</p></blockquote>

<p>You will find an explanation from <a href="http://wiki.apache.org/solr/FAQ#What_does_.22PERFORMANCE_WARNING:_Overlapping_onDeckSearchers.3DX.22_mean_in_my_logs.3F">solr wiki</a> page:</p>

<blockquote><p>This warning means that at least one searcher hadn't yet finished warming in the background, when a commit was issued and another searcher started warming. This can not only eat up a lot of ram (as multiple on deck searches warm caches simultaneously) but it can can create a feedback cycle, since the more searchers warming in parallel means each searcher might take longer to warm.</p>

<p>Typically the way to avoid this error is to either reduce the frequency of commits, or reduce the amount of warming a searcher does while it's on deck (by reducing the work in newSearcher listeners, and/or reducing the autowarmCount on your caches)</p>

<p>See also the <maxWarmingSearchers/> option in SolrConfig.xml.</p></blockquote>

<p>I want to add addtional information before we start analyzing. The warning was always there in production log. We saw around double size this warning type entries comparing with the ones day before.</p>

<h2>Too early conclusion</h2>

<p>From the graphite screenshot I thought it must be something wrong with solr itself. The things which we can control might only be solr configure and JVM. After playing all those two factors and several round tsung stress testings, I kept getting this warning.
I failed to get rid of this waring, which put me back to find more articles about this issue.</p>

<h2>Conclusion</h2>

<p>It was very possible that we used it wrong which was obvious if I had paied more attention to <code>...to avoid this error is to either reduce the frequency of commits, or reduce the amount of warming a searcher does...</code>.</p>

<ul>
<li>Solr is not for realtime search;</li>
<li>Solr is not for replacement of RDB(e.g. mysql);</li>
<li>Solr rely on much of RAM if you have a big index size;</li>
<li><code>Read</code>(<code>Search</code>) and <code>Write</code>(<code>Update</code>) should be treated differently;</li>
</ul>


<p>So the solution could be batch the <code>write</code> requests.</p>

<p>Solrj client provides <a href="https://lucene.apache.org/solr/4_7_2/solr-solrj/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.html">ConcurrentUpdateSolrServer</a> which contained a queue. So we can queued all requests firstly with <code>commitWithin</code> enabled.</p>

<p>More tips about how to optimize your index performance if you also have the <code>write</code> issue:</p>

<ul>
<li><a href="http://wiki.apache.org/lucene-java/ImproveIndexingSpeed">How to make indexing faster</a></li>
<li><a href="http://wiki.apache.org/solr/SolrPerformanceFactors">SolrPerformanceFactors</a></li>
</ul>



				<div class="post-metadata">
					April 20, 2014 | 
					<a href='/en/2014/04/20/solr-performance-issue-solving-process.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#solr'>solr</a>
						, 
					
						
						<a href='/tags.html#solrj'>solrj</a>
						, 
					
						
						<a href='/tags.html#tsung'>tsung</a>
						, 
					
						
						<a href='/tags.html#performance'>performance</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2012/12/09/visit-appspot">
				<a href="/en/2012/12/09/visit-appspot.html"><h1 class="title">Visit Appspot.com From China</h1></a>
				
				<p>The reason behind this blog is to show you how to make <b>GAppProxy</b> work again. In China, most of people rely on <a href='https://code.google.com/p/gappproxy/'>GAppProxy</a> to access the 'outside world'. But appspot.com is blocked in China. So here is a simple tutorial to show you how to make it work.</p>

<p>The idea is very simple: find the workable google IP and add it to hosts file.</p>

<p><br />
Step1:</p>

<pre>
nslookup www.google.com
</pre>


<p>You can find something like:</p>

<pre>
nslookup www.google.com
Server:   192.168.1.1
Address:  192.168.1.1#53

Non-authoritative answer:
Name: www.google.com
Address: 74.125.128.99
Name: www.google.com
Address: 74.125.128.105
Name: www.google.com
Address: 74.125.128.103
Name: www.google.com
Address: 74.125.128.147
Name: www.google.com
Address: 74.125.128.106
Name: www.google.com
Address: 74.125.128.104
</pre>


<p>Basically you just need to try https://74.125.128.x/ to check which one is working. 'x' can be any number. If you can visit https://74.125.128.x/, continue to <b>Step2</b>.</p>

<p><br />
Step2:</p>

<p>Modify /etc/hosts file and add one line:</p>

<pre>
74.125.128.x $your_app_engine_id.appspot.com
</pre>


<p>Replace $your_app_engine_id with your registered google app engine ID.</p>

<p>Hope this will help you.</p>


				<div class="post-metadata">
					December 09, 2012 | 
					<a href='/en/2012/12/09/visit-appspot.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#google'>google</a>
						, 
					
						
						<a href='/tags.html#gfw'>gfw</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2012/10/02/move-to-dnspod">
				<a href="/en/2012/10/02/move-to-dnspod.html"><h1 class="title">Move From Godaddy DNS to Dnspod</h1></a>
				
				<p>It's the first time that my domain can not be resolved for around 6 hours. I learned <a href='http://www.godaddy.com'>godaddy</a> DNS was blocked in China sometimes.</p>

<p>This is what I hate to work on a Chinese site. After several minutes struggle, I decide to move my dns from <a href='http://www.godaddy.com'>godaddy</a> to <a href='https://www.dnspod.cn/'>dnspod</a>.</p>

<p><a href='https://www.dnspod.cn/'>dnspod</a> did really a nice work. No matter the user interfaces or the services. All of them  meet my requirements.</p>

<p>I don't want to repeat how to setup the dnspod. <a href='https://www.google.com/search?q=dns+godaddy+dnspod&oq=dns+godaddy+dnspod&sugexp=chrome,mod=1&sourceid=chrome&ie=UTF-8'>google search</a> will tell you more about this.</p>

<p>Now I will just wait for the new dns takes affect. Actaully dns was solved quickly in China, but not in JiangSu province in my testing. Good luck to my <a href='http://www.dachebang.com'>www.dachebang.com</>.</p>


				<div class="post-metadata">
					October 02, 2012 | 
					<a href='/en/2012/10/02/move-to-dnspod.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#dns'>dns</a>
						
					 
				</div>

			</div>
		
			<div class="post" id="postid-/en/2012/08/26/linode-migration">
				<a href="/en/2012/08/26/linode-migration.html"><h1 class="title">Linode Facility Migration to Fremont</h1></a>
				
				<p>After using <a href='www.linode.com'>linode</a> host center at Tokyo for a while, I decided to migrate my node to Fremont.</p>

<p>Linode provide you a <a href='http://www.linode.com/speedtest/'>speedtest page</a></p>

<p>Here is my speed testing result. Those two tasks are started at the same time.</p>

<pre>
100MB-tokyo.bin 1.2/100MB, 47 min left
100MB-fremont.bin 66.5/100MB, 22 secs left
</pre>


<p>But If I ping the speed test domain:</p>

<pre>
$ ping speedtest.tokyo.linode.com
PING speedtest.tokyo.linode.com (106.187.96.148): 56 data bytes
64 bytes from 106.187.96.148: icmp_seq=0 ttl=51 time=82.885 ms
64 bytes from 106.187.96.148: icmp_seq=1 ttl=51 time=77.757 ms
64 bytes from 106.187.96.148: icmp_seq=2 ttl=51 time=77.919 ms

$ ping speedtest.fremont.linode.com
PING speedtest.fremont.linode.com (50.116.14.9): 56 data bytes
64 bytes from 50.116.14.9: icmp_seq=0 ttl=51 time=146.268 ms
Request timeout for icmp_seq 1
64 bytes from 50.116.14.9: icmp_seq=2 ttl=51 time=146.403 ms
64 bytes from 50.116.14.9: icmp_seq=3 ttl=51 time=145.409 ms
64 bytes from 50.116.14.9: icmp_seq=4 ttl=51 time=146.396 ms
64 bytes from 50.116.14.9: icmp_seq=5 ttl=51 time=148.467 ms
</pre>


<p>As you can see the result is totaly different. So which one you can trust?</p>

<p>The response from linode support:</p>

<pre>
Individual packets take less time to travel between our Tokyo datacenter and your location than our Fremont datacenter, but that our Fremont datacenter is able to provide you with faster download speeds to your location. Which location is better for you depends on whether your use case likes decreased latency, or better download speeds.
</pre>


<p>At last I decide to switch to Fremont anyhow. So Linode provide me a new ip address. The interesting thing comes. My ISP blocked this ip address for some reason. You can find the fail point by using either "traceroute" or <a href='http://library.linode.com/linux-tools/mtr/'>MTR</a>.</p>

<p>So I asked for a new ip address and reboot my node.</p>

<p>Don't forget to update your hosts file with new ip address to make your application work again. Good luck!</p>


				<div class="post-metadata">
					August 26, 2012 | 
					<a href='/en/2012/08/26/linode-migration.html#disqus_thread'>View Comments</a>
					
						
							| 
						
						<a href='/tags.html#linode'>linode</a>
						, 
					
						
						<a href='/tags.html#vps'>vps</a>
						, 
					
						
						<a href='/tags.html#fremont'>fremont</a>
						
					 
				</div>

			</div>
		
		
		<div class="paging">
		
			<a href="/page2" class="previous">&#x25C4; Previous</a>
		
		
		</div>

  </ul>
</div>
</div>

    </div>

	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-9434024-1', 'auto');
	  ga('send', 'pageview');

	</script>
  </body>
</html>
